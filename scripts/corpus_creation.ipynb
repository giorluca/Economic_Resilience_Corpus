{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# API endpoints\n",
    "ENDPOINT = 'https://api.core.ac.uk/v3/search/works/'\n",
    "headers = {'Authorization': 'Bearer gi18bstBoavqIYL9uNnzG2kp6lU3WmRd'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(ENDPOINT, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    rate_limit_remaining = response.headers.get('X-RateLimit-Remaining')\n",
    "    rate_limit_retry_after = response.headers.get('X-RateLimit-Retry-After')\n",
    "    rate_limit_limit = response.headers.get('X-RateLimit-Limit')\n",
    "\n",
    "    print(f\"Rate Limit Remaining: {rate_limit_remaining}\")\n",
    "    print(f\"Rate Limit Retry After: {rate_limit_retry_after}\")\n",
    "    print(f\"Rate Limit Limit: {rate_limit_limit}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearPublished_start = '2019'\n",
    "\n",
    "params_search = {\n",
    "    'q' : rf\"(yearPublished>={yearPublished_start}) AND (language:English) AND (title:resilience+economics OR title:economics+resilience OR title:economic+resilience OR title:resilience+regional OR title:regional+resilience OR title:resilience+shock OR title:shock+resilience OR title:resilience+shocks OR title:shocks+resilience OR title:economic+adaptation OR title:economic+recovery)\",\n",
    "    'limit' : 2000,\n",
    "    'offset' : 0\n",
    "}\n",
    "\n",
    "response = requests.get(ENDPOINT, params=params_search, headers=headers)\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['totalHits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "titles = []\n",
    "authors = []\n",
    "years = []\n",
    "abstracts = []\n",
    "dois = []\n",
    "languages = []\n",
    "publishers = []\n",
    "full_texts = []\n",
    "\n",
    "for i, paper in tqdm(enumerate(data['results']), total=len(data['results'])):\n",
    "    if paper['fullText'] is not None:\n",
    "        full_texts.append(paper['fullText'])\n",
    "    else:\n",
    "        continue\n",
    "    ids.append(paper.get('id', None))\n",
    "    titles.append(paper.get('title', None))\n",
    "    authors.append([author.get('name', None) for author in paper.get('authors', None)])\n",
    "    years.append(paper.get('yearPublished', None))\n",
    "    abstracts.append(paper.get('abstract', None))\n",
    "    dois.append(paper.get('doi', None))\n",
    "    languages.append(paper.get('language', None))\n",
    "    publishers.append(paper.get('publisher', None))\n",
    "\n",
    "\n",
    "corpus = pd.DataFrame({\n",
    "    'COREid':ids,\n",
    "    'title':titles,\n",
    "    'author(s)':authors,\n",
    "    'publication_year':years,\n",
    "    'abstract':abstracts,\n",
    "    'doi':dois,\n",
    "    'lang':languages,\n",
    "    'publisher':publishers,\n",
    "    'text':full_texts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_unique_clean = corpus.drop_duplicates(subset='title').dropna(subset=['title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_unique_clean['title_text'] = [f'{i}\\n\\n{j}' for i,j in zip(corpus_unique_clean['title'], corpus_unique_clean['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "corpus_unique_clean_nostop = corpus_unique_clean\n",
    "corpus_unique_clean_nostop['title_text_nostop'] = corpus_unique_clean_nostop['title_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_unique_clean_nostop.to_csv(f'economic_resilience_corpus_{yearPublished_start}-2024_{len(corpus_unique_clean)}.tsv', sep='\\t', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(f'economic_resilience_corpus_{yearPublished_start}-2024_{len(corpus_unique_clean_nostop)}.tsv') \n",
    "\n",
    "output_dir = f'corpus_txt/{yearPublished_start}-2024_txt'\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    filename = f\"{row['id']}_{row['COREid']}_{row['publication_year']}.txt\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"{row['title_text_nostop']}\")\n",
    "\n",
    "print(f\"Saved {len(df)} files in {output_dir} directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "dist = dict(df['publication_year'].value_counts())\n",
    "\n",
    "# Extract years and counts\n",
    "years = list(dist.keys())\n",
    "counts = list(dist.values())\n",
    "\n",
    "# Normalize counts for color intensity\n",
    "norm_counts = np.array(counts) / max(counts)  # Normalize to [0, 1]\n",
    "\n",
    "# Create color gradient based on normalized counts\n",
    "colors = plt.cm.Blues(norm_counts)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(years, counts, color=colors)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Number of Papers')\n",
    "plt.xticks(years)\n",
    "\n",
    "# Add text labels on top of the bars with larger font size\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, int(yval), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adjust the font size as per your preference (e.g., fontsize=10)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from io import BytesIO\n",
    "import PyPDF2\n",
    "\n",
    "def pdf_to_tsv_from_url(pdf_url):\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf_content = BytesIO(response.content)\n",
    "\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_content)\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    full_text = \"\"\n",
    "    for i in range(num_pages):\n",
    "        page = pdf_reader.pages[i]\n",
    "        full_text += page.extract_text()\n",
    "    \n",
    "    return full_text.replace('\\n', ' ')      \n",
    "      \n",
    "        try:\n",
    "            #download_pdf(paper['downloadUrl'], save_path=f'/home/luca/Desktop/download/n{i}_ID{paper['id']}.pdf')\n",
    "            full_texts.append(pdf_to_tsv_from_url(paper['downloadUrl']))\n",
    "        except:\n",
    "            try:\n",
    "                #download_pdf(paper['sourceFulltextUrls'][0], save_path=f'/home/luca/Desktop/download/n{i}_ID{paper['id']}.pdf')\n",
    "                full_texts.append(pdf_to_tsv_from_url(paper['sourceFulltextUrls'][0]))\n",
    "            except:\n",
    "                print('No working download link. Skipping.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
